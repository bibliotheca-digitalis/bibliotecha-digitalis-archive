start	end	text
0	20960	Yeah, I know Jon and I have, have you watched Nickatine2's recent videos on ethics where
20960	22560	he's sort of battling himself?
22560	24120	Yeah, I like that.
24120	28680	I mean that's, that's brilliant, the almost schizophrenic multi-personality I think is
28680	35560	a good, just underlying kind of way to look at things, it's more complexity than initially
35560	38200	things seem to be, even in ourselves, so.
38200	45200	So yeah, I guess we could start with, with ethics, I mean I know you, you're an atheist
45200	51240	and you obviously think that there's a way to preserve morality and ethics independent
51240	53440	of any theological worldview, right?
53440	56080	Yeah, I think so.
56080	60920	So how do you, how do you ground your ethical system, granted that there's some ambiguity
60920	65520	there for you in light of these last two videos, but, you know, how do you approach the thing?
65520	69440	Well, I mean I know you hate doing this, but suppose we were to start in sort of a place
69440	77480	of Cartesian skepticism, where we, where we know what it feels like to have desires, to
77480	86480	feel pleasure, to feel pain, and we can infer that other people have similar sensations,
86480	91520	that other people have desires and wishes and can feel pain and suffering just as we
91520	97800	can, and we can empathize with them, we can, we have the imaginative power to put ourselves
97800	102960	in their place and act in a way that benefits not only our own selves, our own interests,
102960	105120	but also the interests of other minds.
105120	106980	Right, yeah.
106980	110580	So empathy then would be the basis of, of morality?
110580	112380	Yeah, that's pretty much it.
112380	113380	Okay.
113380	117220	And you know, you said we have to begin with a sort of Cartesian skepticism to write.
117220	118980	We don't have to, but we could.
118980	119980	Right.
119980	125220	Well, I mean, it almost seemed to me though that it's sort of, while Descartes sort of
125220	131060	skepticism involves reflection, I think empathy with others is sort of immediate, you know,
131060	134940	when you see someone who's in pain, you just feel their pain, you know, not literally.
134940	135940	Yeah, yeah, yeah.
135940	136940	I mean, it's much more immediate than that.
136940	137940	You're right.
137940	141300	I mean, Descartes' skepticism is, I mean, I don't, he didn't even believe that for a
141300	142300	second.
142300	143300	That was just his method.
143300	149100	You know, that's something you only come to through thinking about it a lot.
149100	154140	Immediately, there's no question that other people have experiences like your own.
154140	155140	Right.
155140	156140	Yeah.
156140	157140	I would hope not.
157140	158140	But, you know.
158140	159140	Yeah, I would hope not.
159140	164260	But there's something to reflecting on it, you know, after all said and done, after the
164260	168380	immediate event is done, you have time to go back and sip a little wine and reflect
168380	169380	on it.
169380	173700	Is there something to that dimension that complexifies and matures your empathy?
173700	176140	Is it, would you say that's accurate, Nick?
176140	177140	Yeah.
177140	178140	Yeah.
178140	179140	Yeah.
179140	183980	Well, I mean, maybe we could just make a distinction between ethics and morality where ethics is
183980	188300	sort of the practical action in the day-to-day world and then when you come up with a moral
188300	192360	philosophy, that's when you start reflecting on, wait a minute, maybe that ethical decision
192360	198320	I made in the heat of battle wasn't exactly as moral as it could have been.
198320	199320	That's right.
199320	204800	Well, you know, when you're in day-to-day life, you don't always have a chance to like
204800	209120	reflect on what the most moral action might be in that situation.
209120	211120	Well, yeah.
211120	213880	To sit there for now, we're thinking about whether you're going to give someone a transplant
213880	214880	or so.
214880	215880	Yeah.
215880	216880	Right.
216880	217880	You don't have time to think about these things.
217880	218880	And I like, we're talking about ethics.
218880	222280	I like how movies, especially a movie we just saw District 9 on, I don't know if you've
222280	223280	seen that Nick yet.
223280	228720	It ultimately comes out to be a very strong ethical movie.
228720	233120	There's lots of questions amongst all this action and some comical scenes, there are
233120	238560	some really ethical dilemmas in the movie, which I think is good that's promoted in a,
238560	240360	it's a popular kind of meme.
240360	245800	So is it infiltrating pop culture these days, ethics, like through movies like The Matrix,
245800	248440	movies like District 9, is that the way to do it?
248440	250440	Is our sound okay?
250440	251440	Yeah.
251600	255120	I think there's usually, yeah, a lot of movies have some kind of ethical message in them.
255120	260240	I mean, even like the most named superhero movie usually makes some attempt at some kind
260240	261880	of ethical situation.
261880	266720	Well, I remember the most recent, The Dark Knight, the Batman movie, had a bunch of stuff
266720	267720	in that.
267720	270880	Like there was one scene I remember where the Joker has these two boats full of people,
270880	275800	one's all prisoners and one's all just regular people and he's trying to get them to blow
275800	276800	each other up or something.
276800	277800	Right.
277800	278800	Yeah.
278840	283560	And then you see people getting ready to, getting a trigger, and then you actually see
283560	287880	the criminal do one of the most ethical things and he throws it out, you know, we're the
287880	288880	bad guys.
288880	289880	Right.
289880	293720	Yeah, so they're trying to, I mean, I guess it's the skillful means, is that right?
293720	296800	The right director knows how to do it the right way, I suppose?
296800	297800	Yeah.
297800	298800	Right.
298800	299800	Yeah.
299800	300800	Okay.
300800	306960	Well, so I guess, where else can we go with ethics?
306960	308360	Ethics is a big thing.
308360	313000	We don't have to abandon it so far, I mean, a lot of people get into serious arguments
313000	316080	about ethics and when do we put in relativism?
316080	321160	How big of a postmodern kind of world view do you adapt?
321160	324800	What's healthy in your view before it goes to mushy, mushy, gushy, gushy?
324800	330480	Well, I mean, that's a difficult question because you do have these striking examples
330480	334440	of other cultures where things are completely different, like am I allowed, for example,
334440	341720	to condemn someone, an Islamic extremist who kills their own daughter because she left
341720	345920	the religion or something and I almost can't help condemning them, but on the other hand...
345920	350280	That seems like an easier question to answer, I mean, when it's taking someone's life, but
350280	352920	what about like, you know, the headscarves and whatnot?
352920	355360	Do you think women should be explored dressed like women?
355360	362360	Oh, yeah, well, that stuff, I mean, that's relatively minor, I mean, I don't have any
362360	364760	huge objection to women wearing headscarves.
364760	371120	The burqa is a little more up in the air because a lot of times the women wearing those things
371120	375560	are not doing so their own, you know, they're not making a fashion statement, they have
375560	378720	a man and a life enforcing this.
378720	383800	Where was it in, was it Lebanon recently that a woman was wearing pants in public and she...
383800	386360	Yeah, was sentenced to death wearing pants, right.
386360	388360	Was it death or just lashings or something?
388360	393840	No, it was like the lashing equivalent of a death sentence, like 40 lashings or something.
393840	394840	Oh, yeah.
394840	397640	There's a part of you that just says, what the fuck is going on?
397640	398640	That's ridiculous.
398640	403640	I think that's, I mean, but see, just if we can all agree that that's ridiculous, another
403640	408600	moral question is, should we actually go to Lebanon invading it and actually try to change
408600	409600	it?
409600	414000	That's, I'll cover up with Professor Anton actually is the concept of, and you're going
414000	419880	to get a lot of people hissing and moaning at this, a one-world type of governing system
419880	420880	body.
420880	426160	But it's also, to try to fix the problem would cause, at least initially, more violence and
426160	429480	death than the problem itself.
429480	430480	This is also seen in movies.
430480	432360	I mean, more absolute power, would that corrupt?
432360	433360	Absolutely.
433360	436160	I mean, I think, I've seen a video where you've actually quoted that and tried to explain
436160	438600	about that, but it just seems like, what do you do?
438600	443480	This is just this dilemma, do we just let them be or do we go absolute, Ron Paul, libertarian,
443960	448600	you know, you guys go for it, how about it, you know?
448600	453480	So what, I mean, how do you deal with some of these questions where you see the truth,
453480	457160	the grain of truth that they bring to the perspective?
457160	461080	Like there's a party that nods, like, yeah, it makes sense, but at the same time, it conflicts.
461080	462080	This is strange.
462080	463080	Yeah.
463080	469000	Well, let me ask you, Nikitin, do you think that there is in any way a good, a capital
469000	474280	G good that's greater than any particular cultural good that sort of has authority over
474280	477720	what any particular world you may think is good?
477720	480480	Or do you think that that's just an illusion?
480480	481640	I don't think so.
481640	483200	I don't think that exists.
483200	488240	I think good is a creation of our own minds and our own minds are, of course, running
488240	491080	a lot of cultural software that are influencing it.
491080	497760	I mean, I think there are some basic ideas that are so a part of who we are, biologically
497760	504160	speaking, that pretty much all of us, with the exception of, say, sociopaths, have these
504160	511880	intuitions and feelings, but I don't think there's like a good floating out in the way.
511880	512880	Yeah, yeah, yeah.
512880	513880	Right.
513880	514880	Right.
514880	518400	Well, I think someone else watching this might interpret that, like, you take the stance
518400	519400	of amendum.
519400	522680	He thinks there's an objective, you know, it's a fact of suffering, so not suffering
522680	524600	would be objectively good for him.
524600	525600	Right.
525600	530000	And you try to say, well, that does a subjective spin, not that it's wrong, or an intersubjective
530000	535320	spin, but he says, no, you're fucked-hearted, it has to be, it is an objective kind of thing.
535320	536320	Yeah, you're right.
536320	537320	Yeah.
537320	538320	Amendum, he said suffering is fucking objective.
538320	543320	You know, I've never been able to figure out what he means by that, because to me, it seems
543320	548840	like suffering, pleasure, color, all of that is fairly subjective.
548840	551640	So I can't even figure out what he's trying to say.
551680	557840	I agree with the underlying message that he's trying to convey, just not the way he tries
557840	558840	to convey it.
558840	564880	I usually agree with the inclusions he draws, but the way he comes to conclusions is weird.
