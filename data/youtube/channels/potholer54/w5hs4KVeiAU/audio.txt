Climate change is a natural phenomenon that has always been, but always will be.
Climate does change. It always has, and it always will.
Yes, OK, the climate changes. In science that's what's called an observation,
just as we can observe lightning, shooting stars and earthquakes.
They're all natural. But the role of scientists isn't to sit back and marvel at all these natural mysteries.
It's to find out why they happen.
So what climatologists want to know is what causes the climate to change.
And after two centuries of research, they now have a pretty good idea.
Let's just recap.
Insulation is the amount of energy we get from the Sun.
It depends not only on the energy output of the Sun,
but on how the Earth is positioned to receive it, its tilt and orbit.
Greenhouse gases, things like water vapour, carbon dioxide and methane.
And thirdly, particulates and aerosols. They come from volcanoes and meteorite impacts.
They block sunlight and cool the Earth down,
and aerosols caused by industrial pollution do the same thing.
Finally, the Earth itself can amplify these changes through positive feedback,
and even the configuration of the continents.
And so we come to our first urban myth,
which happened to pop up in the comments forum of one of my climate videos.
The myth is that since temperatures were higher in the past with lower CO2,
and sometimes lower with higher CO2,
carbon dioxide can't be a factor in climate change.
Even Bob Carter has fallen for this one.
Here's a graph he produced, apparently showing no correlation
between carbon dioxide and global temperatures
over the Phanerozoic, the last 500 million years of the Earth's history.
But I'm pretty sure on this one, Bob,
because it's in all the scientific literature.
You wouldn't expect to get a match-up
because carbon dioxide isn't the only factor that affects global temperatures.
No climatologist ever said it was. Just read the journals.
On a geological time scale, the Sun is the other main driver of climate.
And as every geologist should know,
solar output was much weaker in the deep past.
The red line here shows solar output throughout the Earth's history.
What we're interested in is the last 500 million years.
Like most stars, the Sun has been growing increasingly brighter with time.
If I compared solar irradiance with global temperatures
over the last half billion years,
I'd get no better a correlation than Carter did with carbon dioxide.
But if we combine the two, solar irradiation and carbon dioxide,
then there's a very good correlation.
I'll come back to this later,
but why is the correlation so good if there are all these other factors?
Well, the Earth's tilt and orbit are predictable and run in cycles.
The effect of particulates is usually short-lived.
And the concentration of water vapour is controlled by temperature,
so it acts as an amplifier rather than a prime driver of climate.
Over the long term, carbon dioxide and solar output, or irradiance,
are the biggest variables.
Let me put this very simply with an analogy.
Let's say you're staying in a house in mid-winter
and all you have to heat it with is a fireplace and an electric heater.
If they're both full-on, you'll bake,
and if they're both turned down very low, you'll freeze.
The house could even get cold if the fire's going nicely,
but the heater's turned down very low.
So the Earth's climate depends on a combination of factors,
and it's actually quite lucky for us
that carbon dioxide levels have generally fallen over geological time
to balance the growing irradiance of the Sun.
Two researchers who are sceptical of the influence of CO2
have put forward an alternative idea
that the Earth cools as it passes through the spiral arms of our galaxy.
The idea is that cosmic rays ionise the air and that seeds clouds,
and clouds reflect sunlight, and that cools the Earth down.
Shivivam Veda's paper has been widely circulated
but not widely accepted within the scientific community.
No, not because of some conspiracy of evil scientists,
but because the idea that cosmic rays seed clouds
and that clouds cool the climate hasn't been shown.
I explained the background to this in my video
Climate Change the Objections.
Also, geologists say there are problems with the dating method used
and with the correlation itself.
And as the authors themselves admit,
the hypothesis can't explain current climate change
because it takes millions of years for the Solar System
to pass through a spiral arm.
I'm very glad a couple of qualified sceptics who know their subject
chose to publish their research in a respected peer-reviewed journal.
That's how scientific procedure is supposed to work.
Unfortunately, Shiviv's university issued a press release at the same time
the paper was published, making claims that weren't in there.
Most journalists read the press release
and didn't bother to read the paper.
So the headline that went around the world was this one,
even though the paper says nothing of the sort.
In fact, it specifically argues against drawing inferences
about current climate change.
Shivivam Veda go on to say that other research suggests
cosmic rays may also work on a smaller time scale,
but that hypothesis has since collapsed
because of a mathematical error,
which I explained in my video, Climate Change, the Objections.
So what we're left with are two very clear drivers of climate
that fit very well with the reconstruction of the Earth's climatic history.
From 2004 to 2006, researchers reviewed the scientific literature
and examined solar output, carbon dioxide levels and temperature data
from every period of the Phanerozoic,
from the Tertiary to the lower Ordovician.
And they found a very good correlation.
Critics say the correlation breaks down in two places,
the Ordovician and the Cretaceous periods,
when there were very high CO2 levels coinciding with extensive ice cover.
But of course we have to remember the climate isn't driven by carbon dioxide alone,
and at times even the combined influence of solar output and CO2 levels
can be upset by the other factors that affect climate.
The Ordovician seems to be a particular favourite with amateur sceptics
because it experienced a full-blown ice age.
I'm often referred to this website, run not by a climatologist, no surprise,
but by a mining engineer called Monty Hebe.
To the consternation of global warming proponents,
the late Ordovician period was also an ice age,
while at the same time CO2 concentrations then were nearly 12 times higher than today.
According to greenhouse theory, Earth should have been exceedingly hot, blah blah blah.
No, according to greenhouse theory, if you want to call it that,
global temperatures vary depending on several factors, not just carbon dioxide.
So let's go back over 20 years of research into the Ordovician climate
and see what real climate scientists have discovered.
First of all, solar output during the Ordovician was, of course,
much weaker than today, about 4.5% weaker.
That should have been enough to plunge the Earth into an ice age.
It certainly would today.
But in fact the climate was much hotter,
way beyond any forecast of what we're expecting in the next 100 years.
How can that be?
Well, paleoclimatologists have concluded it's because carbon dioxide levels
were around 18 times higher than today.
So if all that carbon dioxide was keeping the Earth hot,
why did it suddenly descend into an ice age?
This was a problem facing geologists over a decade ago.
The only way to cool the Earth down would have been to remove
billions of tonnes of carbon dioxide from the atmosphere.
But there was no known mechanism for that happening
over such a short period of time.
In 1994 geologists discovered that carbon dioxide levels
did indeed drop dramatically towards the end of the Ordovician.
The reason still wasn't clear.
Climatologists calculated that such a drop
would have caused global temperatures to fall,
but there was still more than enough CO2 in the atmosphere
to prevent an ice age.
Then in 1995 researchers from Texas A&M showed that
one other factor had come into play,
the position of the continents,
which had joined into a supercontinent near the South Pole.
They also concluded that the Earth's orbit might have played a role
in reducing solar irradiance even further.
Two years later researchers from Pennsylvania State University
calculated that all these factors put together,
including the drop in carbon dioxide levels,
would have been enough to precipitate an ice age.
Then in 1999 and 2005,
newspapers began to shed light on the reason carbon dioxide levels fell.
During the Ordovician, the Atlantic Oceanic crust
was subducting under the North American continent.
This thrust up the Appalachian Mountains.
As they rose, the silicate rocks in these newly formed mountains rapidly weathered.
This happens through a simple chemical process.
Carbon dioxide reacts with silicate rock,
forming calcium or magnesium carbonate.
This is washed away into the sea,
gets taken up by organisms in their shells,
and then gets buried as sediment that turns into limestone or chalk.
So the carbon that was once in the atmosphere got buried at sea,
and over millions of years, trillions of tons of carbon were removed from the air.
So scientists are more than happy to talk about this U-boat 21.
In fact, they've been yakking about it for well over two decades.
It's just that you haven't noticed
because it's all been published in scientific journals.
These aren't inaccessible. You can find them quite easily online.
But they don't get passed around and re-pasted on internet blogs.
They require a bit of effort.
I suppose it's quicker and easier to read a blog by a mining engineer
and come away thinking you're an instant expert.
It's kind of the fast-food approach to science.
So all the Mc-experts who argue that the Earth was cold
when carbon dioxide levels were high
miss the equally irrelevant point that the Earth was much hotter than today
when solar output was much lower than today.
It's a combination of factors that drives our climate.
But a lot of people continue to think that the sun is acting alone.
None of the major climate changes in the last thousand years
can be explained by CO2.
Well, I can beat that.
None of the major climate changes in the last 1,150 years
can be explained by CO2.
And there's a very good reason for that.
CO2 levels have been more or less constant during that time.
As we saw with the example of the house in winter,
if one fire is constant, then the temperature rises and falls
according to the output of the other.
A study that looked at global temperatures over the last 1,150 years
found exactly that.
With carbon dioxide levels steady, the sun was the main driver of climate.
But for the last 30 years, the output of the sun has been more or less constant.
So as climatologists predicted, temperatures have risen concurrently
with the rise in the other historical driver of climate, carbon dioxide.
And to the other MOOC experts that make the obvious and rather puerile observation
that the climate always changes, well, now you know why.
